{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBc5DAmSDZzLKWY+QbDqcL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/376_Causal_Inference_for_Data_Scientists.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/gG8h5gtCGOY"
      ],
      "metadata": {
        "id": "XWROCXQo3VgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Causal Inference for Data Scientists: Moving from Association to Intervention**\n",
        "\n",
        "## **Introduction: The Fundamental Problem with Traditional ML**\n",
        "\n",
        "When we build machine learning models for marketing, we typically ask: \"Which customers are likely to convert?\" But this is the WRONG question for business decisions!\n",
        "\n",
        "The RIGHT question is: \"Which customers will convert BECAUSE of our marketing campaign?\"\n",
        "\n",
        "**Example:**\n",
        "- Your model predicts a customer has 80% conversion probability\n",
        "- Should you target them with an expensive ad campaign?\n",
        "- **You can't answer this without causal inference!**\n",
        "\n",
        "Why? Because the 80% might mean:\n",
        "1. They'll convert anyway (Sure Thing - waste of money)\n",
        "2. They'll convert ONLY with the ad (Persuadable - good ROI)\n",
        "3. They won't convert regardless (Lost Cause - waste of money)\n",
        "\n",
        "Traditional ML (including SHAP) tells us about **ASSOCIATION**: \"Treatment is correlated with conversion\"\n",
        "Causal inference tells us about **INTERVENTION**: \"Treatment CAUSES conversion\"\n",
        "\n",
        "**This tutorial shows you the difference and why it matters for marketing ROI.**\n",
        "\n",
        "**Note:** By the way, Causal inference isn’t just for marketing - it’s fundamental in many fields where decisions or interventions matter.\n",
        "\n",
        "For example, in healthcare, a predictive model might say a patient has a high chance of recovery, but that doesn’t tell us whether a treatment actually causes the recovery. Causal inference helps answer the real question: “Will this patient recover because of the treatment?” This distinction is critical for evaluating drug effectiveness, treatment policies, and clinical decision-making.\n",
        "\n",
        "## **What is Causal Inference?**\n",
        "\n",
        "Causal inference answers the question: \"What would happen if we changed something?\"\n",
        "\n",
        "**The Gold Standard: Randomized Controlled Trial (RCT)**\n",
        "<br>(Kind of like A/B testing)\n",
        "- Randomly assign customers to treatment (show ad) or control (no ad)\n",
        "- Compare outcomes between groups\n",
        "- The difference is the causal effect\n",
        "\n",
        "## **The Challenge: Observational Data**\n",
        "\n",
        "**The Problem in Plain English:**\n",
        "\n",
        "Imagine you're trying to figure out if your ads actually work. You look at your data and see:\n",
        "- Customers who saw ads: 5% conversion rate\n",
        "- Customers who didn't see ads: 2% conversion rate\n",
        "\n",
        "You might think: \"Great! My ads triple conversions!\" But wait...\n",
        "\n",
        "**Here's what's really happening:**\n",
        "\n",
        "Your ad platform is smart. It shows ads to people who:\n",
        "- Visit your website frequently\n",
        "- Click on things\n",
        "- Search for products like yours\n",
        "- Have bought from you before\n",
        "\n",
        "In other words, **you're showing ads to people who were already interested!**\n",
        "\n",
        "**A Real-World Analogy:**\n",
        "\n",
        "Think of it like a gym trying to prove their program works:\n",
        "- People who join the gym: 80% get fit\n",
        "- People who don't join: 20% get fit\n",
        "\n",
        "Does this prove the gym works? NO! Because:\n",
        "- People who join gyms are ALREADY motivated to get fit\n",
        "- They might exercise at home, eat healthy, etc.\n",
        "- They'd probably get fit even WITHOUT the gym\n",
        "\n",
        "The gym membership and fitness are both caused by a third thing: **motivation**\n",
        "\n",
        "**Back to Marketing:**\n",
        "\n",
        "Same problem with ads:\n",
        "- High-engagement users see more ads (algorithm targets them)\n",
        "- High-engagement users convert more (they're interested)\n",
        "- **But they might convert EVEN WITHOUT the ads!**\n",
        "\n",
        "**This is confounding:**\n",
        "```\n",
        "          Engagement Level\n",
        "               /     \\\n",
        "              /       \\\n",
        "             ↓         ↓\n",
        "       Sees Ad    →  Converts\n",
        "```\n",
        "\n",
        "Engagement affects BOTH who sees ads AND who converts. This creates a fake connection between ads and conversions.\n",
        "\n",
        "**What we really want to know:**\n",
        "\n",
        "\"If I took a random person and showed them an ad vs. not showing them an ad, what would happen?\"\n",
        "\n",
        "**Causal inference helps by:**\n",
        "- Finding similar people who saw vs. didn't see ads\n",
        "- Accounting for differences in engagement, interests, etc.\n",
        "- Estimating what WOULD have happened if we could run a perfect experiment\n",
        "\n",
        "**Simple Example:**\n",
        "\n",
        "- Customer A: High engagement, saw ad, converted\n",
        "- Customer B: High engagement, NO ad, also converted\n",
        "\n",
        "This tells us: For high-engagement customers, the ad didn't matter - they convert anyway!\n",
        "\n"
      ],
      "metadata": {
        "id": "a3-1z5CxiyfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Criteo Dataset**\n",
        "\n",
        "I'm using the Criteo Uplift Prediction Dataset - real advertising data with:\n",
        "- **Treatment**: User was shown an ad (1) or not shown (0)\n",
        "- **Outcome**: User converted (1) or didn't (0)\n",
        "- **Features**: 12 anonymized user characteristics (f0-f11)\n",
        "\n",
        "This is perfect for causal analysis because it comes from actual A/B tests (randomized experiments).\n",
        "\n",
        "For more information: https://ailab.criteo.com/criteo-uplift-prediction-dataset/"
      ],
      "metadata": {
        "id": "G5gJq6lik8ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latest xgboost giving some issues when working with shap, so rollingback.\n",
        "!pip install xgboost==2.0.3\n",
        "\n",
        "#For causal inference....\n",
        "!pip install dowhy\n",
        "\n",
        "# I'll load the Criteo uplift dataset using sklift library\n",
        "# This dataset contains real advertising campaign data with treatment and control groups\n",
        "!pip install scikit-uplift"
      ],
      "metadata": {
        "id": "3QwNTCQobUM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and explore data**\n",
        "\n",
        "I'm loading the Criteo advertising dataset which contains real campaign data. The key variables are:\n",
        "\n",
        "- **treatment**: Whether the user was shown an ad (1) or not (0)\n",
        "- **conversion**: Whether the user made a purchase (1) or not (0)\n",
        "- **f0-f11**: 12 anonymized features about the user (behavioral, demographic, etc.)\n",
        "\n",
        "**Critical data cleaning:**\n",
        "Machine learning models and causal inference methods are sensitive to missing or invalid data, so I'm:\n",
        "1. Removing any rows with NaN or infinite values\n",
        "2. Ensuring treatment and outcome are binary (0 or 1)\n",
        "3. Using a manageable 100,000 sample for this tutorial\n",
        "\n",
        "**Key observations from the data:**\n",
        "- Treatment is heavily imbalanced: 85% treated, 15% control\n",
        "- Conversions are rare: only 0.33% conversion rate (typical for advertising)\n",
        "- Control group converts at 0.16%, treated group at 0.36%\n",
        "- Simple difference: 0.19 percentage points\n",
        "\n",
        "**Question to ponder:** Is this 0.19 percentage point difference the TRUE causal effect of the ad? Or is it biased by confounding? We'll find out!"
      ],
      "metadata": {
        "id": "1qg2mccDi-yc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3edAWPjNalzj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "\n",
        "\n",
        "\n",
        "from sklift.datasets import fetch_criteo\n",
        "\n",
        "# Load the dataset - I'll use 10% for faster computation\n",
        "dataset = fetch_criteo(target_col='conversion', treatment_col='treatment', percent10=True, return_X_y_t=True)\n",
        "X, y, treatment = dataset\n",
        "\n",
        "print(f\"Full dataset shape: {X.shape}\")\n",
        "\n",
        "# Convert to pandas Series for easier manipulation\n",
        "y = pd.Series(y, name='conversion')\n",
        "treatment = pd.Series(treatment, name='treatment')\n",
        "\n",
        "# Comprehensive data cleaning\n",
        "print(f\"\\nData Cleaning:\")\n",
        "print(f\"Initial rows: {len(X)}\")\n",
        "\n",
        "# Step 1: Check for missing values\n",
        "print(f\"\\nMissing values in X: {X.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in y: {y.isnull().sum()}\")\n",
        "print(f\"Missing values in treatment: {treatment.isnull().sum()}\")\n",
        "\n",
        "# Step 2: Remove rows with any missing values\n",
        "valid_mask = ~(X.isnull().any(axis=1) | y.isnull() | treatment.isnull())\n",
        "X = X[valid_mask].reset_index(drop=True)\n",
        "y = y[valid_mask].reset_index(drop=True)\n",
        "treatment = treatment[valid_mask].reset_index(drop=True)\n",
        "\n",
        "print(f\"After removing NaN: {len(X)} rows\")\n",
        "\n",
        "# Step 3: Check for infinite values\n",
        "inf_mask = np.isinf(X.select_dtypes(include=[np.number]).values).any(axis=1)\n",
        "print(f\"Rows with infinite values: {inf_mask.sum()}\")\n",
        "\n",
        "if inf_mask.sum() > 0:\n",
        "    X = X[~inf_mask].reset_index(drop=True)\n",
        "    y = y[~inf_mask].reset_index(drop=True)\n",
        "    treatment = treatment[~inf_mask].reset_index(drop=True)\n",
        "    print(f\"After removing inf: {len(X)} rows\")\n",
        "\n",
        "# Step 4: Ensure valid values for y and treatment (should be 0 or 1)\n",
        "valid_y = y.isin([0, 1])\n",
        "valid_treatment = treatment.isin([0, 1])\n",
        "valid_binary = valid_y & valid_treatment\n",
        "\n",
        "X = X[valid_binary].reset_index(drop=True)\n",
        "y = y[valid_binary].reset_index(drop=True)\n",
        "treatment = treatment[valid_binary].reset_index(drop=True)\n",
        "\n",
        "print(f\"After ensuring binary values: {len(X)} rows\")\n",
        "\n",
        "# Step 5: Final validation\n",
        "assert X.isnull().sum().sum() == 0, \"Still have NaN in X\"\n",
        "assert y.isnull().sum() == 0, \"Still have NaN in y\"\n",
        "assert treatment.isnull().sum() == 0, \"Still have NaN in treatment\"\n",
        "assert not np.isinf(X.values).any(), \"Still have inf in X\"\n",
        "print(\"\\n✓ Data is clean!\")\n",
        "\n",
        "# For this tutorial, I'll use a manageable subset for consistent analysis\n",
        "SAMPLE_SIZE = 100000\n",
        "if len(X) > SAMPLE_SIZE:\n",
        "    sample_indices = np.random.RandomState(42).choice(len(X), size=SAMPLE_SIZE, replace=False)\n",
        "    X = X.iloc[sample_indices].reset_index(drop=True)\n",
        "    y = y.iloc[sample_indices].reset_index(drop=True)\n",
        "    treatment = treatment.iloc[sample_indices].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nUsing {len(X)} samples for this tutorial\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"\\nFeature names: {list(X.columns)}\")\n",
        "\n",
        "# Let me check the treatment distribution\n",
        "print(f\"\\nTreatment distribution:\")\n",
        "print(treatment.value_counts())\n",
        "print(\"1 = Treated (showed ad), 0 = Control (no ad)\")\n",
        "\n",
        "# Check outcome distribution\n",
        "print(f\"\\nConversion distribution:\")\n",
        "print(y.value_counts())\n",
        "print(\"1 = Converted, 0 = Did not convert\")\n",
        "\n",
        "# Create a combined dataframe for analysis\n",
        "df = X.copy()\n",
        "df['treatment'] = treatment.values\n",
        "df['conversion'] = y.values\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Plot distributions\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Treatment distribution\n",
        "treatment.value_counts().plot(kind='bar', ax=ax1, color=['blue', 'red'])\n",
        "ax1.set_title('Treatment Distribution')\n",
        "ax1.set_xlabel('Treatment (0=Control, 1=Treated)')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_xticklabels(['Control', 'Treated'], rotation=0)\n",
        "\n",
        "# Conversion distribution\n",
        "y.value_counts().plot(kind='bar', ax=ax2, color=['gray', 'green'])\n",
        "ax2.set_title('Conversion Distribution')\n",
        "ax2.set_xlabel('Conversion (0=No, 1=Yes)')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.set_xticklabels(['No Conversion', 'Conversion'], rotation=0)\n",
        "\n",
        "# Conversion rate by treatment\n",
        "conversion_by_treatment = df.groupby('treatment')['conversion'].mean()\n",
        "conversion_by_treatment.plot(kind='bar', ax=ax3, color=['blue', 'red'])\n",
        "ax3.set_title('Conversion Rate by Treatment')\n",
        "ax3.set_xlabel('Treatment')\n",
        "ax3.set_ylabel('Conversion Rate')\n",
        "ax3.set_xticklabels(['Control', 'Treated'], rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nConversion rate in Control group: {conversion_by_treatment[0]:.4f}\")\n",
        "print(f\"Conversion rate in Treated group: {conversion_by_treatment[1]:.4f}\")\n",
        "print(f\"Simple difference (Treated - Control): {conversion_by_treatment[1] - conversion_by_treatment[0]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train XGBoost model (traditional approach)**\n",
        "\n",
        "This is the traditional machine learning approach: train a model to predict conversion using all available features, including treatment.\n",
        "\n",
        "**What this tells us:** (based on the results we get from this code block)\n",
        "- Model accuracy: 99.7% (impressive, but misleading because conversions are so rare)\n",
        "- AUC: 0.97 (actually good - model distinguishes converters from non-converters well)\n",
        "- Feature importance shows f4, f11, f8 are most predictive\n",
        "\n",
        "**Notice:** Treatment ranks 10th out of 13 features in importance (0.0298). This seems low!\n",
        "\n",
        "**The Problem:**\n",
        "This model tells us \"what features predict conversion\" but NOT \"what features CAUSE conversion.\" The treatment importance score is about ASSOCIATION, not CAUSATION.\n",
        "\n",
        "If we use this model to decide who to target, we might:\n",
        "- Waste money on \"sure things\" who convert anyway\n",
        "- Miss \"persuadables\" who need the ad to convert\n",
        "- Target \"do not disturbs\" who are actually harmed by ads"
      ],
      "metadata": {
        "id": "M1GeVwTTjDR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traditional approach: train a model to predict conversion using all features including treatment\n",
        "# This is what most people do - but it doesn't tell us the CAUSAL effect of treatment\n",
        "\n",
        "# Create a dataframe that includes everything for easy splitting\n",
        "df_with_treatment = X.copy()\n",
        "df_with_treatment['treatment'] = treatment\n",
        "df_with_treatment['conversion'] = y\n",
        "\n",
        "# Split the data\n",
        "train_df, test_df = train_test_split(df_with_treatment, test_size=0.2, random_state=42, stratify=df_with_treatment['conversion'])\n",
        "\n",
        "# Separate features, treatment, and outcome\n",
        "X_train_with_treatment = train_df.drop('conversion', axis=1)\n",
        "y_train = train_df['conversion']\n",
        "\n",
        "X_test_with_treatment = test_df.drop('conversion', axis=1)\n",
        "y_test = test_df['conversion']\n",
        "\n",
        "print(f\"Training set size: {X_train_with_treatment.shape[0]}\")\n",
        "print(f\"Test set size: {X_test_with_treatment.shape[0]}\")\n",
        "\n",
        "# Train XGBoost model\n",
        "model = xgb.XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
        "model.fit(X_train_with_treatment, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test_with_treatment)\n",
        "y_pred_proba = model.predict_proba(X_test_with_treatment)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train_with_treatment.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 features by importance:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_features = feature_importance.head(10)\n",
        "plt.barh(top_features['feature'], top_features['importance'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 10 Features - XGBoost')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nYAQZspfarj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHAP explanations (association, not causation)**\n",
        "\n",
        "Now I'm using SHAP to explain individual predictions. SHAP answers: \"Why did the model predict this person would convert?\"\n",
        "\n",
        "**For Sample 10:** (just a random sample)\n",
        "- Treatment: Shown ad (1)\n",
        "- Prediction: 0.05% conversion probability (very low)\n",
        "- SHAP shows which features pushed this prediction up or down\n",
        "\n",
        "**The waterfall plot shows:**\n",
        "Starting from baseline prediction, each feature contributes. We can see exactly how the model arrived at its prediction.\n",
        "\n",
        "**CRITICAL LIMITATION:**\n",
        "SHAP tells us \"treatment is associated with higher conversion predictions\"\n",
        "\n",
        "SHAP does NOT tell us \"if we changed this person's treatment from 0 to 1, their conversion probability would increase by X\"\n",
        "\n",
        "**The distinction:**\n",
        "- SHAP: \"People who saw ads are more likely to convert\" (observation)\n",
        "- Causal: \"Showing ads MAKES people more likely to convert\" (intervention)\n",
        "\n",
        "For marketing decisions (should we spend money on ads?), we need the causal answer!"
      ],
      "metadata": {
        "id": "kc7fDYzbjGX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let me use SHAP to understand feature contributions\n",
        "# But remember: SHAP shows ASSOCIATION, not CAUSATION\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer(X_train_with_treatment.iloc[:1000])  # Using subset for speed\n",
        "\n",
        "print(f\"Base value (baseline prediction): {explainer.expected_value:.4f}\")\n",
        "\n",
        "# Summary plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values, X_train_with_treatment.iloc[:1000], show=False)\n",
        "plt.title('SHAP Summary Plot - Association between Features and Conversion')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Let me pick one sample and explain it\n",
        "sample_idx = 10\n",
        "sample = X_test_with_treatment.iloc[sample_idx:sample_idx+1]\n",
        "shap_values_sample = explainer(sample)\n",
        "\n",
        "pred_proba = model.predict_proba(sample)[0]\n",
        "print(f\"\\nSample {sample_idx}:\")\n",
        "print(f\"Treatment: {sample['treatment'].values[0]} ({'Treated' if sample['treatment'].values[0] == 1 else 'Control'})\")\n",
        "print(f\"Actual conversion: {y_test.iloc[sample_idx]}\")\n",
        "print(f\"Predicted conversion probability: {pred_proba[1]:.4f}\")\n",
        "\n",
        "# Waterfall plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.plots.waterfall(shap_values_sample[0], show=False)\n",
        "plt.title('SHAP Explanation - Why This Prediction?')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCRITICAL QUESTION: Does this tell us the CAUSAL effect of treatment?\")\n",
        "print(\"NO! SHAP shows that 'treatment' is associated with conversion.\")\n",
        "print(\"But it doesn't answer: What if we CHANGED this person's treatment status?\")\n",
        "print(\"That's where causal inference comes in...\")"
      ],
      "metadata": {
        "id": "oFSIO3kKavJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let me use **DoWhy** to estimate the CAUSAL effect of treatment\n"
      ],
      "metadata": {
        "id": "DrkNUryJazAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to causal inference with DoWhy**\n",
        "\n",
        "Now I'm using DoWhy (Microsoft's causal inference library) to estimate the TRUE causal effect of treatment.\n",
        "\n",
        "**The Causal Graph:**\n",
        "I'm specifying the causal relationships:\n",
        "- Treatment → Conversion (this is what we want to estimate)\n",
        "- Features → Treatment (features affect who gets treated)\n",
        "- Features → Conversion (features affect who converts)\n",
        "\n",
        "This means features are **confounders** - they create spurious correlation between treatment and outcome.\n",
        "\n",
        "**Example of confounding:**\n",
        "- High-engagement users (f0=high) are more likely to:\n",
        "  - Be shown ads (algorithm targets engaged users)\n",
        "  - Convert anyway (they're already interested)\n",
        "- If we don't adjust for f0, we'll overestimate the ad's causal effect\n",
        "\n",
        "**DoWhy's approach:**\n",
        "1. **Identify**: What assumptions do we need? (all confounders observed)\n",
        "2. **Estimate**: Use statistical methods to remove confounding bias\n",
        "3. **Refute**: Test if the estimate is robust\n",
        "\n",
        "### **Still confused?**"
      ],
      "metadata": {
        "id": "ox6HdusEjKLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding the Problem with a Simple Example**\n",
        "\n",
        "Before diving into our advertising data, let me explain causal inference with an everyday example.\n",
        "\n",
        "**Question: Does coffee cause productivity?**\n",
        "\n",
        "You observe:\n",
        "- Coffee drinkers: 80% productive\n",
        "- Non-coffee drinkers: 50% productive\n",
        "\n",
        "Can you conclude coffee causes +30% productivity? **NO!** Here's why:\n",
        "\n",
        "**Morning people:**\n",
        "- Wake up early → Have time to drink coffee\n",
        "- Wake up early → More productive (fresh start, quiet time)\n",
        "\n",
        "So \"morning person\" is a **confounder** - it affects both coffee drinking AND productivity:\n",
        "```\n",
        "    Morning Person\n",
        "        ↓         ↓\n",
        "      Coffee  →  Productivity\n",
        "```\n",
        "\n",
        "Morning people both drink more coffee AND are more productive. The direct coffee→productivity effect might be much smaller (or zero!). The 30% difference is misleading because it includes the confounding effect of being a morning person.\n",
        "\n",
        "**What we really want to know:**\n",
        "\"If I took the SAME person and gave them coffee vs no coffee, what would happen?\"\n",
        "\n",
        "This is the causal question. To answer it, we need to **adjust for confounders** like \"morning person.\"\n",
        "\n",
        "## **Our Advertising Problem is Identical**\n",
        "\n",
        "In our ad campaign data:\n",
        "- Treated customers (saw ad): 0.36% conversion\n",
        "- Control customers (no ad): 0.16% conversion\n",
        "- Simple difference: 0.19 percentage points\n",
        "\n",
        "But wait! Who sees ads?\n",
        "- High-engagement customers (algorithm targets them)\n",
        "- Customers who browse frequently\n",
        "- Customers with high purchase intent\n",
        "\n",
        "These same customers are ALSO more likely to convert even without ads!\n",
        "```\n",
        "    Customer Engagement (f0-f11)\n",
        "           ↓              ↓\n",
        "       Sees Ad    →   Converts\n",
        "```\n",
        "\n",
        "The features (f0-f11) are confounders - they affect both who gets treated AND who converts.\n",
        "\n",
        "## **The Causal Model: Telling DoWhy What We Know**\n",
        "```python\n",
        "model_causal = CausalModel(\n",
        "    data=df_causal,\n",
        "    treatment='treatment',         # What we control (show ad or not)\n",
        "    outcome='conversion',          # What we want to affect\n",
        "    common_causes=list(X.columns)  # Confounders (affect both treatment and outcome)\n",
        ")\n",
        "```\n",
        "\n",
        "I'm telling DoWhy three things:\n",
        "\n",
        "1. **Treatment = 'treatment'**: This is what we can intervene on (show ad or not)\n",
        "2. **Outcome = 'conversion'**: This is what we want to increase\n",
        "3. **Common causes = f0-f11**: These are confounders that affect BOTH who sees ads AND who converts\n",
        "\n",
        "**The causal graph DoWhy creates:**\n",
        "```\n",
        "       f0, f1, f2, ... f11 (features)\n",
        "        ↓              ↓\n",
        "    Treatment  →  Conversion\n",
        "```\n",
        "\n",
        "This graph says: \"All features affect both treatment and outcome. We need to adjust for them to isolate the true treatment→conversion effect.\"\n",
        "\n",
        "PS: You can also directly pass a *networkx.DiGraph* to the CausalModel."
      ],
      "metadata": {
        "id": "bMqE5-7vCo6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dowhy\n",
        "from dowhy import CausalModel\n",
        "\n",
        "# Prepare data for DoWhy\n",
        "df_causal = X.copy()\n",
        "df_causal['treatment'] = treatment\n",
        "df_causal['conversion'] = y\n",
        "\n",
        "# Define the causal model\n",
        "# I need to specify the causal graph (what causes what)\n",
        "model_causal = CausalModel(\n",
        "    data=df_causal,\n",
        "    treatment='treatment',\n",
        "    outcome='conversion',\n",
        "    common_causes=list(X.columns)  # All features could confound treatment and outcome\n",
        ")\n",
        "\n",
        "# Visualize the causal graph\n",
        "print(\"Causal Graph:\")\n",
        "print(\"Treatment → Conversion\")\n",
        "print(\"Features (f0-f11) → Treatment (confounders)\")\n",
        "print(\"Features (f0-f11) → Conversion (confounders)\")\n",
        "print(\"\\nThis means: features might affect both who gets treated AND who converts\")\n",
        "print(\"We need to adjust for these confounders to get the true causal effect\")\n",
        "\n",
        "# View the causal model\n",
        "model_causal.view_model()\n",
        "from IPython.display import Image\n",
        "try:\n",
        "    Image(filename=\"causal_model.png\")\n",
        "except:\n",
        "    print(\"(Graph visualization requires graphviz)\")"
      ],
      "metadata": {
        "id": "2tYUIVGzd12v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Three Key Steps: Identify, Estimate, and Refute**\n",
        "*Note: Refute is in the next code block.*\n",
        "\n",
        "**Step 1: Identify (Can we do this?)**\n",
        "\n",
        "DoWhy checks: \"Given this causal graph and data, CAN we estimate the causal effect?\"\n",
        "\n",
        "It looks for:\n",
        "- Do we have the right variables measured?\n",
        "- Is there a statistical method that works?\n",
        "- What assumptions do we need?\n",
        "\n",
        "**Output:** \"Yes! Use the backdoor adjustment method - adjust for all features f0-f11\"\n",
        "\n",
        "Think of identification as: **Getting the recipe**\n",
        "\n",
        "**Step 2: Estimate (Actually calculate it)**\n",
        "\n",
        "DoWhy then uses statistical methods (linear regression, matching, etc.) to compute the actual number.\n",
        "\n",
        "**Output:** \"Average Treatment Effect = 0.00169\"\n",
        "\n",
        "Think of estimation as: **Following the recipe to get the answer**\n",
        "\n",
        "## **Back to the Coffee Example**\n",
        "\n",
        "**Identify step says:**\n",
        "\"To find the causal effect of coffee, compare coffee drinkers vs non-drinkers who have the SAME morning person score and sleep quality.\"\n",
        "\n",
        "**Estimate step calculates:**\n",
        "\"After adjusting for morning person and sleep quality, coffee increases productivity by only 5% (not 30%!)\"\n",
        "\n",
        "The 5% is the TRUE causal effect. The other 25% was due to confounding (morning people drink more coffee and are more productive).\n",
        "\n",
        "## **What This Means for Our Ad Campaign**\n",
        "\n",
        "Instead of just comparing treated vs control groups (which gives us 0.195pp - percentage points), DoWhy will:\n",
        "\n",
        "1. Find treated and control customers with SIMILAR feature values (f0-f11)\n",
        "2. Compare their conversion rates\n",
        "3. This removes the confounding bias\n",
        "4. Gives us the TRUE causal effect of the ad\n",
        "\n",
        "In the next block, we'll see that the true causal effect (0.169pp) is actually 13% smaller than the simple difference (0.195pp) because of confounding bias!\n",
        "\n",
        "\n",
        "**So, in this block of code, I'm estimating the TRUE causal effect of showing ads.**\n",
        "\n",
        "**Three Methods (all should give similar answers if done right):**\n",
        "\n",
        "**1. Linear Regression (Backdoor Adjustment):**\n",
        "- Estimate: 0.169 percentage points\n",
        "- Method: Regress conversion on treatment + all confounders\n",
        "- Interpretation: Showing ads causes 0.169pp increase in conversions\n",
        "\n",
        "**2. Propensity Score Matching:**\n",
        "- Estimate: 0.206 percentage points  \n",
        "- Method: Match treated users with similar control users, compare outcomes\n",
        "- Intuition: Find \"twins\" who differ only in treatment\n",
        "\n",
        "**3. Propensity Score Stratification:**\n",
        "- Failed due to parameter issue (happens sometimes)\n",
        "\n",
        "**KEY COMPARISON:**\n",
        "- Simple difference (no adjustment): 0.195pp\n",
        "- Causal effect (adjusted): 0.169pp\n",
        "- **Confounding bias: 0.026pp**\n",
        "\n",
        "The simple difference OVERESTIMATES the true causal effect by 13%! This is because users who saw ads were already more likely to convert (confounding).\n",
        "\n",
        "**Business Impact:**\n",
        "If you calculate ROI using the simple difference, you'll think ads are 13% more effective than they actually are. This could lead to overspending on ineffective campaigns!"
      ],
      "metadata": {
        "id": "SyCaDvMIjMsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before creating causal model, let me ensure data quality\n",
        "print(\"Data Quality Check:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Any NaN: {df.isnull().any().any()}\")\n",
        "print(f\"Any Inf: {np.isinf(df.select_dtypes(include=[np.number])).any().any()}\")\n",
        "\n",
        "# Double-check data types\n",
        "df_causal = df.copy()\n",
        "# Ensure treatment and conversion are integers\n",
        "df_causal['treatment'] = df_causal['treatment'].astype(int)\n",
        "df_causal['conversion'] = df_causal['conversion'].astype(int)\n",
        "# Ensure all features are float\n",
        "for col in X.columns:\n",
        "    df_causal[col] = df_causal[col].astype(float)\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df_causal.dtypes)\n",
        "\n",
        "# Create causal model with our cleaned data\n",
        "model_causal = CausalModel(\n",
        "    data=df_causal,\n",
        "    treatment='treatment',\n",
        "    outcome='conversion',\n",
        "    common_causes=list(X.columns)\n",
        ")\n",
        "\n",
        "print(f\"\\nUsing {len(df_causal)} samples for causal analysis\")\n",
        "print(f\"Treatment distribution: \\n{df_causal['treatment'].value_counts()}\")\n",
        "print(f\"Conversion rate: {df_causal['conversion'].mean():.4f}\")\n",
        "\n",
        "# Step 1: Identify the causal effect\n",
        "identified_estimand = model_causal.identify_effect(proceed_when_unidentifiable=True)\n",
        "print(\"\\nIdentification Strategy:\")\n",
        "print(identified_estimand)\n",
        "\n",
        "# Step 2: Estimate the causal effect using multiple methods\n",
        "\n",
        "# Method 1: Linear Regression (backdoor adjustment) - try this first as it's most stable\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CAUSAL EFFECT ESTIMATION - Linear Regression (Backdoor Adjustment)\")\n",
        "print(\"=\"*80)\n",
        "try:\n",
        "    estimate_lr = model_causal.estimate_effect(\n",
        "        identified_estimand,\n",
        "        method_name=\"backdoor.linear_regression\",\n",
        "        test_significance=True\n",
        "    )\n",
        "    print(f\"Average Treatment Effect (ATE): {estimate_lr.value:.6f}\")\n",
        "    print(f\"Interpretation: On average, showing the ad CAUSES a {estimate_lr.value * 100:.4f} percentage point\")\n",
        "    print(\"increase in conversion probability\")\n",
        "except Exception as e:\n",
        "    print(f\"Linear regression failed: {e}\")\n",
        "    estimate_lr = None\n",
        "\n",
        "# Method 2: Propensity Score Matching\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CAUSAL EFFECT ESTIMATION - Propensity Score Matching\")\n",
        "print(\"=\"*80)\n",
        "try:\n",
        "    estimate_psm = model_causal.estimate_effect(\n",
        "        identified_estimand,\n",
        "        method_name=\"backdoor.propensity_score_matching\"\n",
        "    )\n",
        "    print(f\"Average Treatment Effect (ATE): {estimate_psm.value:.6f}\")\n",
        "    print(f\"\\nInterpretation: On average, showing the ad CAUSES a {estimate_psm.value * 100:.4f} percentage point\")\n",
        "    print(\"increase in conversion probability (compared to not showing the ad)\")\n",
        "except Exception as e:\n",
        "    print(f\"PSM failed: {e}\")\n",
        "    estimate_psm = None\n",
        "\n",
        "# Method 3: Propensity Score Stratification (more robust than weighting)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CAUSAL EFFECT ESTIMATION - Propensity Score Stratification\")\n",
        "print(\"=\"*80)\n",
        "try:\n",
        "    estimate_strat = model_causal.estimate_effect(\n",
        "        identified_estimand,\n",
        "        method_name=\"backdoor.propensity_score_stratification\",\n",
        "        num_strata=5\n",
        "    )\n",
        "    print(f\"Average Treatment Effect (ATE): {estimate_strat.value:.6f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Stratification failed: {e}\")\n",
        "    estimate_strat = None\n",
        "\n",
        "# Compare with simple difference (no causal adjustment)\n",
        "simple_diff = df_causal.groupby('treatment')['conversion'].mean().diff().iloc[-1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Simple difference (no adjustment): {simple_diff:.6f}\")\n",
        "if estimate_lr:\n",
        "    print(f\"Causal effect (Linear Reg): {estimate_lr.value:.6f}\")\n",
        "    print(f\"Bias from confounding: {abs(simple_diff - estimate_lr.value):.6f}\")\n",
        "if estimate_psm:\n",
        "    print(f\"Causal effect (PSM): {estimate_psm.value:.6f}\")\n",
        "if estimate_strat:\n",
        "    print(f\"Causal effect (Stratification): {estimate_strat.value:.6f}\")\n",
        "\n",
        "if estimate_lr:\n",
        "    print(\"\\n✓ Successfully estimated causal effect!\")\n",
        "else:\n",
        "    print(\"\\n⚠ Could not estimate causal effect with these methods\")"
      ],
      "metadata": {
        "id": "7RpaCluBawkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Refutation tests (validate causal findings)**\n",
        "\n",
        "Just because I calculated a causal estimate doesn't mean it's correct! I need to validate it.\n",
        "\n",
        "**Three Robustness Checks:**\n",
        "\n",
        "**1. Random Common Cause Test:**\n",
        "- Add a random variable that shouldn't matter\n",
        "- Result: Estimate barely changed (0.00169 → 0.00169)\n",
        "- ✓ Pass: Good! Random noise doesn't affect the estimate\n",
        "\n",
        "**2. Placebo Treatment Test:**\n",
        "- Replace real treatment with random \"fake\" treatment\n",
        "- Result: Effect drops to near zero (0.00001)\n",
        "- ✓ Pass: Perfect! When treatment is random, effect disappears\n",
        "\n",
        "**3. Data Subset Validation:**\n",
        "- Re-estimate on random 80% subsets\n",
        "- Result: Estimate stable (0.00153 average vs 0.00169 original)\n",
        "- ✓ Pass: Effect is consistent across different samples\n",
        "\n",
        "**Conclusion:**\n",
        "All three tests pass! This gives us confidence that:\n",
        "1. The causal effect is real (not a statistical artifact)\n",
        "2. The estimate is stable (not sensitive to small data changes)\n",
        "3. We're measuring true causation (not just correlation)\n",
        "\n",
        "**What if tests failed?**\n",
        "- Might indicate unmeasured confounding\n",
        "- Need better data or different methods\n",
        "- Be cautious about making business decisions"
      ],
      "metadata": {
        "id": "YUyHq-9LjPtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me validate the causal estimate using refutation tests\n",
        "# I'll use the linear regression estimate as it's most stable\n",
        "\n",
        "print(\"REFUTATION TESTS - Validating Causal Estimates\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Testing estimate: {estimate_lr.value:.6f}\")\n",
        "\n",
        "# Test 1: Random Common Cause\n",
        "print(\"\\n1. Random Common Cause Test:\")\n",
        "print(\"   Adding a random variable that shouldn't affect anything...\")\n",
        "refute_random = model_causal.refute_estimate(\n",
        "    identified_estimand,\n",
        "    estimate_lr,\n",
        "    method_name=\"random_common_cause\"\n",
        ")\n",
        "print(f\"   New estimate: {refute_random.new_effect:.6f}\")\n",
        "print(f\"   Original estimate: {refute_random.estimated_effect:.6f}\")\n",
        "print(f\"   Pass: Estimate should remain similar\" if abs(refute_random.new_effect - refute_random.estimated_effect) < 0.001 else \"   ✗ Fail\")\n",
        "\n",
        "# Test 2: Placebo Treatment\n",
        "print(\"\\n2. Placebo Treatment Test:\")\n",
        "print(\"   Replacing real treatment with random treatment...\")\n",
        "refute_placebo = model_causal.refute_estimate(\n",
        "    identified_estimand,\n",
        "    estimate_lr,\n",
        "    method_name=\"placebo_treatment_refuter\",\n",
        "    placebo_type=\"permute\"\n",
        ")\n",
        "print(f\"   New estimate: {refute_placebo.new_effect:.6f}\")\n",
        "print(f\"   Original estimate: {refute_placebo.estimated_effect:.6f}\")\n",
        "print(f\"   Pass: Placebo effect should be near zero\" if abs(refute_placebo.new_effect) < 0.001 else f\"   Note: Placebo effect = {refute_placebo.new_effect:.6f}\")\n",
        "\n",
        "# Test 3: Data Subset Validation\n",
        "print(\"\\n3. Data Subset Validation Test:\")\n",
        "print(\"   Checking if effect is stable across data subsets...\")\n",
        "refute_subset = model_causal.refute_estimate(\n",
        "    identified_estimand,\n",
        "    estimate_lr,\n",
        "    method_name=\"data_subset_refuter\",\n",
        "    subset_fraction=0.8,\n",
        "    num_simulations=5\n",
        ")\n",
        "print(f\"   Mean estimate across subsets: {refute_subset.new_effect:.6f}\")\n",
        "print(f\"   Original estimate: {refute_subset.estimated_effect:.6f}\")\n",
        "print(f\"   Pass: Effect stable across subsets\" if abs(refute_subset.new_effect - refute_subset.estimated_effect) < 0.001 else \"   ✗ Note: Some variation across subsets\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSION:\")\n",
        "if abs(refute_placebo.new_effect) < 0.001:\n",
        "    print(\"Causal effect appears robust! Treatment has a real causal impact.\")\n",
        "else:\n",
        "    print(\"Some sensitivity detected. Consider additional robustness checks.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "pG2NAS9ubAhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHAP vs Causal - Side by side comparison**\n",
        "\n",
        "Let me directly compare what SHAP tells us vs what causal inference tells us.\n",
        "\n",
        "**SHAP (Association):**\n",
        "- Treatment ranks 10th in feature importance\n",
        "- Importance score: 0.0298 (relatively low)\n",
        "- Average absolute SHAP value: 0.0648\n",
        "- **Says:** \"Treatment is somewhat associated with conversion\"\n",
        "- **Doesn't say:** \"What happens if we change treatment?\"\n",
        "\n",
        "**Causal Inference (Intervention):**\n",
        "- Average Treatment Effect: 0.169 percentage points\n",
        "- **Says:** \"Showing ads CAUSES 0.169pp increase in conversion\"\n",
        "- **Tells us:** The actual ROI if we run this campaign\n",
        "\n",
        "**The Graph Shows:**\n",
        "- Left: SHAP importance (f4 is most \"important\" for prediction)\n",
        "- Right: Causal effects (simple difference overestimates true effect)\n",
        "\n",
        "**Why the difference matters:**\n",
        "\n",
        "For a **data scientist building a prediction model:**\n",
        "- SHAP is perfect! Shows which features drive predictions\n",
        "- Helps debug models, find biases, explain to stakeholders\n",
        "\n",
        "For a **marketing manager deciding campaign budgets:**\n",
        "- Causal is essential! Shows actual ROI of spending money on ads\n",
        "- Tells you if campaigns are cost-effective\n",
        "- Identifies which customers to target\n",
        "\n",
        "**Real-world example:**\n",
        "- SHAP might say \"previous purchase history\" is very important\n",
        "- But you can't change someone's purchase history!\n",
        "- Causal tells you what you CAN change (send ad, offer discount, etc.)\n",
        "\n",
        "**Bottom line:** Use SHAP for model interpretation, use causal for business decisions."
      ],
      "metadata": {
        "id": "4UAXiSsejSUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me create a clear comparison between SHAP (association) and Causal (intervention)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SHAP (ASSOCIATION) vs CAUSAL INFERENCE (INTERVENTION)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# SHAP: How important is treatment for prediction?\n",
        "treatment_shap_importance = feature_importance[feature_importance['feature'] == 'treatment']['importance'].values[0]\n",
        "\n",
        "# Calculate average absolute SHAP value for treatment\n",
        "treatment_shap_values = []\n",
        "for i in range(min(500, len(X_test_with_treatment))):\n",
        "    shap_val = explainer(X_test_with_treatment.iloc[i:i+1])\n",
        "    treatment_shap_values.append(abs(shap_val.values[0][-1]))\n",
        "\n",
        "avg_treatment_shap = np.mean(treatment_shap_values)\n",
        "\n",
        "print(\"\\nSHAP (Association):\")\n",
        "print(f\"   Feature importance rank: {list(feature_importance['feature']).index('treatment') + 1} out of {len(feature_importance)}\")\n",
        "print(f\"   Importance score: {treatment_shap_importance:.4f}\")\n",
        "print(f\"   Average |SHAP value|: {avg_treatment_shap:.4f}\")\n",
        "print(\"   Interpretation: Treatment is associated with conversion predictions\")\n",
        "print(\"   What SHAP doesn't tell us: What happens if we CHANGE treatment?\")\n",
        "\n",
        "print(\"\\nCausal Inference (Intervention):\")\n",
        "print(f\"   Average Treatment Effect: {estimate_lr.value:.6f}\")\n",
        "print(f\"   Effect size: {estimate_lr.value * 100:.2f} percentage points\")\n",
        "print(\"   Interpretation: Showing the ad CAUSES conversion probability to increase\")\n",
        "print(\"   What causal tells us: The actual impact of changing treatment\")\n",
        "\n",
        "# Visualize the comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# SHAP importance\n",
        "top_10_features = feature_importance.head(10)\n",
        "ax1.barh(top_10_features['feature'], top_10_features['importance'])\n",
        "ax1.set_xlabel('SHAP Feature Importance')\n",
        "ax1.set_title('SHAP: Which Features Predict Conversion?\\n(Association)')\n",
        "ax1.invert_yaxis()\n",
        "\n",
        "# Causal effect - only include estimates we have\n",
        "methods = ['Simple\\nDifference', 'Linear\\nRegression']\n",
        "effects = [simple_diff, estimate_lr.value]\n",
        "colors = ['gray', 'green']\n",
        "\n",
        "if estimate_psm:\n",
        "    methods.insert(1, 'Propensity\\nScore\\nMatching')\n",
        "    effects.insert(1, estimate_psm.value)\n",
        "    colors.insert(1, 'green')\n",
        "\n",
        "if estimate_strat:\n",
        "    methods.append('Propensity\\nStratification')\n",
        "    effects.append(estimate_strat.value)\n",
        "    colors.append('green')\n",
        "\n",
        "ax2.bar(methods, effects, color=colors)\n",
        "ax2.set_ylabel('Treatment Effect on Conversion')\n",
        "ax2.set_title('Causal: What Happens if We Change Treatment?\\n(Intervention)')\n",
        "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY TAKEAWAY:\")\n",
        "print(\"=\"*80)\n",
        "print(\"• SHAP answers: 'What features are associated with the outcome?'\")\n",
        "print(\"• Causal answers: 'What happens if we intervene on a feature?'\")\n",
        "print(\"\\nFor marketing decisions, we need CAUSAL answers:\")\n",
        "print(\"- Should we spend money on this ad campaign? → Causal\")\n",
        "print(\"- Which customers should we target? → Causal (heterogeneous effects)\")\n",
        "print(\"- What's the ROI of our intervention? → Causal\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "gEX2KMnjbD4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Individual treatment effects (heterogeneity)**\n",
        "\n",
        "## **The Problem with Averages**\n",
        "\n",
        "So far we've calculated the **Average Treatment Effect (ATE)**: on average, showing ads causes a 0.169 percentage point increase in conversions.\n",
        "\n",
        "But here's the problem: **Averages hide the story.**\n",
        "\n",
        "**Simple analogy:**\n",
        "If I put one hand in boiling water and one hand in ice water, on average I'm comfortable. But in reality, I'm in pain!\n",
        "\n",
        "**For marketing:**\n",
        "An average treatment effect of +0.169pp could mean:\n",
        "- Everyone gets a small +0.169pp boost (uniform effect)\n",
        "- OR 10% get a huge +1.5pp boost, 90% get nothing (heterogeneous effect)\n",
        "- OR some get +5pp boost, some get -2pp harm (very heterogeneous)\n",
        "\n",
        "These scenarios require VERY different marketing strategies!\n",
        "\n",
        "## **What We're Doing: Individual Treatment Effects (ITE)**\n",
        "\n",
        "Instead of one number for everyone, let me estimate the treatment effect for EACH person individually.\n",
        "\n",
        "**Back to the coffee example:**\n",
        "- Average effect: Coffee increases productivity by 5%\n",
        "- But:\n",
        "  - Morning people: +2% (already productive, coffee helps a little)\n",
        "  - Night owls: +15% (coffee really helps them wake up!)\n",
        "  - Anxious people: -5% (coffee makes them jittery, less productive)\n",
        "\n",
        "If you're selling coffee, you'd target night owls, not morning people or anxious people!\n",
        "\n",
        "## **The Method: S-Learner (Single Model)**\n",
        "\n",
        "Here's how I calculate Individual Treatment Effects:\n",
        "\n",
        "**Step 1:** Use the XGBoost model we trained in Block 2\n",
        "- This model learned the relationship between features, treatment, and conversion\n",
        "- It can predict: \"Given these features and treatment, what's the conversion probability?\"\n",
        "\n",
        "**Step 2:** For EACH person in the test set, predict TWO scenarios:\n",
        "\n",
        "**Scenario A - They see the ad:**\n",
        "```python\n",
        "customer['treatment'] = 1\n",
        "prob_if_treated = model.predict_proba(customer)[0][1]\n",
        "```\n",
        "\n",
        "**Scenario B - They don't see the ad:**\n",
        "```python\n",
        "customer['treatment'] = 0\n",
        "prob_if_control = model.predict_proba(customer)[0][1]\n",
        "```\n",
        "\n",
        "**Step 3:** Calculate the Individual Treatment Effect (ITE):\n",
        "```python\n",
        "ITE = prob_if_treated - prob_if_control\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "- Customer X:\n",
        "  - Conversion probability WITH ad: 2.0%\n",
        "  - Conversion probability WITHOUT ad: 0.5%\n",
        "  - ITE = 2.0% - 0.5% = +1.5 percentage points\n",
        "  - **Interpretation:** This ad increases Customer X's conversion by 1.5pp - they're a persuadable!\n",
        "\n",
        "- Customer Y:\n",
        "  - Conversion probability WITH ad: 0.5%\n",
        "  - Conversion probability WITHOUT ad: 0.5%\n",
        "  - ITE = 0.5% - 0.5% = 0.0 percentage points\n",
        "  - **Interpretation:** The ad doesn't affect Customer Y - no effect!\n",
        "\n",
        "\n",
        "## **The Three Customer Segments**\n",
        "\n",
        "Based on ITE scores, I classify customers into three simple groups:\n",
        "\n",
        "**Persuadables (ITE > 0.001):**\n",
        "- Conversion probability increases meaningfully with ad\n",
        "- These are customers on the fence - the ad tips them over\n",
        "- **Action:** TARGET THESE! This is your ROI.\n",
        "\n",
        "**No Effect (ITE ≈ 0, between -0.001 and +0.001):**\n",
        "- Ad has essentially no impact on conversion\n",
        "- Either they're very unlikely to convert, or they'll convert regardless\n",
        "- **Action:** Don't waste budget. The ad doesn't matter.\n",
        "\n",
        "**Do Not Disturb (ITE < -0.001):**\n",
        "- Ad actually DECREASES conversion probability\n",
        "- Maybe the ad annoys them, creates decision fatigue, or interrupts their flow\n",
        "- **Action:** Actively EXCLUDE from campaigns. You're harming yourself!\n",
        "\n",
        "**The thresholds:**\n",
        "- ITE > 0.001 = +0.1pp or more increase → Meaningful positive effect\n",
        "- ITE < -0.001 = -0.1pp or more decrease → Meaningful negative effect\n",
        "- ITE ≈ 0 = Between -0.1pp and +0.1pp → No meaningful effect\n",
        "\n",
        "## **Key Findings from Our Results**\n",
        "\n",
        "Now let me show you what we found in our 20,000 test customers:\n",
        "\n",
        "**Distribution of treatment effects:**\n",
        "- Average ITE: 0.13pp (close to our ATE of 0.169pp - good validation!)\n",
        "- Standard deviation: 0.99pp (huge variation!)\n",
        "- Range: -16.7pp to +32.5pp (enormous spread!)\n",
        "\n",
        "**Most people cluster near zero** - for 86.5% of customers, ads don't meaningfully affect conversion.\n",
        "\n",
        "**Customer segmentation:**\n",
        "- **Persuadables: 2,544 (12.7%)** - These are your targets!\n",
        "- **No Effect: 17,296 (86.5%)** - Ad doesn't matter for them\n",
        "- **Do Not Disturb: 160 (0.8%)** - Ad actually harms conversion\n",
        "\n",
        "## **The Business Implication**\n",
        "\n",
        "**Traditional targeting:** Show ads to all 20,000 customers\n",
        "- Cost: 20,000 × ad cost\n",
        "- You're wasting money on 87.3% who won't respond\n",
        "\n",
        "**Smart targeting:** Show ads ONLY to 2,544 persuadables\n",
        "- Cost: 2,544 × ad cost (87% savings!)\n",
        "- Lose almost no real conversions (the 17,296 \"no effect\" customers weren't converting because of ads anyway)\n",
        "- **Result: Similar conversions at 13% of the cost = 7.5x ROI improvement**\n",
        "\n",
        "**Real examples from our data:**\n",
        "\n",
        "**Persuadable (Index 5):**\n",
        "- Without ad: 0.10% conversion probability\n",
        "- With ad: 0.23% conversion probability\n",
        "- ITE: +0.13pp\n",
        "- **This person needs the ad to convert. Target them!**\n",
        "\n",
        "**No Effect (Index 0):**\n",
        "- Without ad: 0.03% conversion probability\n",
        "- With ad: 0.04% conversion probability  \n",
        "- ITE: +0.01pp (essentially zero)\n",
        "- **The ad makes no difference. Save your money.**\n",
        "\n",
        "**Do Not Disturb (Index 154):**\n",
        "- Without ad: 1.56% conversion probability\n",
        "- With ad: 1.41% conversion probability\n",
        "- ITE: -0.15pp\n",
        "- **The ad decreases conversion! This person was MORE likely to buy without being interrupted. Actively exclude them.**\n",
        "\n",
        "## **This is the Power of Heterogeneous Treatment Effects**\n",
        "\n",
        "You've moved from:\n",
        "- \"Does advertising work on average?\" (Yes, +0.169pp)\n",
        "- To: \"WHO does advertising work on?\" (Only 12.7% persuadables)\n",
        "\n",
        "This is how you transform marketing from spray-and-pray to precision targeting."
      ],
      "metadata": {
        "id": "KrMbAqpcjVG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Not everyone responds the same to treatment\n",
        "# Let me estimate individual-level treatment effects\n",
        "\n",
        "# Use the test set we created earlier\n",
        "# Create two versions of test data: one with treatment=1, one with treatment=0\n",
        "X_test_treated = X_test_with_treatment.copy()\n",
        "X_test_treated['treatment'] = 1\n",
        "\n",
        "X_test_control = X_test_with_treatment.copy()\n",
        "X_test_control['treatment'] = 0\n",
        "\n",
        "# Predict conversion probability under both scenarios\n",
        "prob_if_treated = model.predict_proba(X_test_treated)[:, 1]\n",
        "prob_if_control = model.predict_proba(X_test_control)[:, 1]\n",
        "\n",
        "# Individual Treatment Effect (ITE) = difference\n",
        "ite = prob_if_treated - prob_if_control\n",
        "\n",
        "print(\"INDIVIDUAL TREATMENT EFFECTS (Heterogeneity)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Number of test samples: {len(ite)}\")\n",
        "print(f\"Average ITE: {ite.mean():.6f}\")\n",
        "print(f\"Std Dev ITE: {ite.std():.6f}\")\n",
        "print(f\"Min ITE: {ite.min():.6f}\")\n",
        "print(f\"Max ITE: {ite.max():.6f}\")\n",
        "\n",
        "# Plot distribution of treatment effects\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of ITEs\n",
        "ax1.hist(ite, bins=50, edgecolor='black', alpha=0.7)\n",
        "ax1.axvline(x=ite.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean ITE: {ite.mean():.4f}')\n",
        "ax1.axvline(x=0.001, color='green', linestyle='--', linewidth=1, alpha=0.7, label='Persuadable threshold')\n",
        "ax1.axvline(x=-0.001, color='red', linestyle='--', linewidth=1, alpha=0.7, label='Do not disturb threshold')\n",
        "ax1.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "ax1.set_xlabel('Individual Treatment Effect')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('Distribution of Individual Treatment Effects')\n",
        "ax1.legend()\n",
        "\n",
        "# Categorize users - SIMPLE THREE CATEGORIES\n",
        "persuadables = (ite > 0.001).sum()\n",
        "do_not_disturb = (ite < -0.001).sum()\n",
        "no_effect = ((ite >= -0.001) & (ite <= 0.001)).sum()\n",
        "\n",
        "categories = ['Persuadables\\n(ITE > 0.001)', 'No Effect\\n(ITE ≈ 0)', 'Do Not Disturb\\n(ITE < -0.001)']\n",
        "counts = [persuadables, no_effect, do_not_disturb]\n",
        "colors_cat = ['green', 'gray', 'red']\n",
        "\n",
        "ax2.bar(categories, counts, color=colors_cat, alpha=0.7)\n",
        "ax2.set_ylabel('Number of Users')\n",
        "ax2.set_title('User Segmentation by Treatment Response')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nUser Segmentation:\")\n",
        "print(f\"  Persuadables (ITE > 0.001): {persuadables} ({persuadables/len(ite)*100:.1f}%)\")\n",
        "print(f\"  Do Not Disturb (ITE < -0.001): {do_not_disturb} ({do_not_disturb/len(ite)*100:.1f}%)\")\n",
        "print(f\"  No Effect (ITE ≈ 0): {no_effect} ({no_effect/len(ite)*100:.1f}%)\")\n",
        "\n",
        "# Verify percentages add up\n",
        "total_pct = (persuadables + do_not_disturb + no_effect) / len(ite) * 100\n",
        "print(f\"\\n  Total: {total_pct:.1f}% (should be 100%)\")\n",
        "\n",
        "print(\"\\nBusiness Insight:\")\n",
        "print(f\"Only target the {persuadables} persuadable users ({persuadables/len(ite)*100:.1f}%) to maximize ROI!\")\n",
        "print(f\"The other {no_effect + do_not_disturb} users ({(no_effect + do_not_disturb)/len(ite)*100:.1f}%) won't respond or will be harmed by ads\")\n",
        "\n",
        "# Show example individuals\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXAMPLES OF INDIVIDUAL TREATMENT EFFECTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find examples of each category\n",
        "if persuadables > 0:\n",
        "    persuadable_idx = np.where(ite > 0.001)[0][0]\n",
        "    print(f\"\\n1. Persuadable (Index {persuadable_idx}):\")\n",
        "    print(f\"   Prob if NOT shown ad: {prob_if_control[persuadable_idx]:.4f}\")\n",
        "    print(f\"   Prob if shown ad: {prob_if_treated[persuadable_idx]:.4f}\")\n",
        "    print(f\"   Treatment Effect: +{ite[persuadable_idx]:.4f}\")\n",
        "    print(f\"   → Should target this user!\")\n",
        "\n",
        "if no_effect > 0:\n",
        "    no_effect_idx = np.where((ite >= -0.001) & (ite <= 0.001))[0][0]\n",
        "    print(f\"\\n2. No Effect (Index {no_effect_idx}):\")\n",
        "    print(f\"   Prob if NOT shown ad: {prob_if_control[no_effect_idx]:.4f}\")\n",
        "    print(f\"   Prob if shown ad: {prob_if_treated[no_effect_idx]:.4f}\")\n",
        "    print(f\"   Treatment Effect: {ite[no_effect_idx]:+.4f}\")\n",
        "    print(f\"   → Ad doesn't matter, don't waste budget\")\n",
        "\n",
        "if do_not_disturb > 0:\n",
        "    dnd_idx = np.where(ite < -0.001)[0][0]\n",
        "    print(f\"\\n3. Do Not Disturb (Index {dnd_idx}):\")\n",
        "    print(f\"   Prob if NOT shown ad: {prob_if_control[dnd_idx]:.4f}\")\n",
        "    print(f\"   Prob if shown ad: {prob_if_treated[dnd_idx]:.4f}\")\n",
        "    print(f\"   Treatment Effect: {ite[dnd_idx]:.4f}\")\n",
        "    print(f\"   → Ad harms conversion, actively exclude!\")"
      ],
      "metadata": {
        "id": "mAIUdBQ-bGB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Steps: Putting This Into Action**\n",
        "\n",
        "Now you know that only 12.7% of customers are persuadables - people who convert BECAUSE of your ads. The other 87% either convert anyway (sure things), won't convert regardless (lost causes), or are actually harmed by ads (do not disturb). So what do you actually do with this information?\n",
        "\n",
        "**Step 1: Build a persuadable scoring system.** Use the uplift modeling approach from this tutorial to score every customer. For each new customer, predict their Individual Treatment Effect (ITE) - the difference between their conversion probability with and without your ad. Customers with high ITE scores are your persuadables. You can use Python libraries like `causalml`, `scikit-uplift`, or `econml` to build these models on your historical campaign data.\n",
        "\n",
        "**Step 2: Stop targeting everyone, start targeting smart.** Instead of showing ads to anyone with a high conversion probability, show ads ONLY to customers with positive treatment effects. In practice: export your customer list with persuadable scores, create custom audiences in Facebook/Google Ads for \"High Uplift Customers,\" and exclude sure things and lost causes as negative audiences. For the 20,000 customers in our test set, this means targeting 2,544 persuadables instead of all 17,000 - an 85% reduction in ad spend while maintaining most conversions (since sure things convert anyway).\n",
        "\n",
        "**Step 3: Track what actually matters.** Stop measuring total conversions and ROAS. Start measuring incremental conversions - conversions you wouldn't have gotten without the ad. Calculate Incremental ROI = (Incremental Revenue - Ad Spend) / Ad Spend. This is your true marketing impact. In our analysis, the \"total\" ROI looked 8x better than the incremental ROI because it incorrectly counted sure things as marketing wins.\n",
        "\n",
        "The key insight: Marketing isn't about reaching the most people. It's about reaching the RIGHT people - those you can actually influence. Use causal inference to find them, and watch your marketing ROI multiply."
      ],
      "metadata": {
        "id": "ODVgYczFjXwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example ITE scoring**"
      ],
      "metadata": {
        "id": "GYHbksCr6ztL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let us create sample new customers**\n",
        "\n",
        "Now let me show you how to apply this in practice. Imagine 5 new customers just visited your website and you need to decide: should you show them ads or not?\n",
        "\n",
        "I'm creating 5 hypothetical customers with different behavioral profiles based on the feature distributions from our training data:\n",
        "\n",
        "- **Customer A**: Low engagement profile (f0=12.6, low activity scores)\n",
        "- **Customer B**: Medium engagement (f0=24.5, moderate activity)\n",
        "- **Customer C**: High engagement with diverse behavior (mixed signals)\n",
        "- **Customer D**: Very high activity (f0=22.0, f9=61.9 shows extreme browsing)\n",
        "- **Customer E**: Outlier profile (unusual feature combinations)\n",
        "\n",
        "These represent the range of customers you'd encounter in real marketing scenarios. Now let's score them to see who's worth targeting."
      ],
      "metadata": {
        "id": "fNPO3D8h64TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me create a few hypothetical new customers to score\n",
        "# I'll use the feature distributions from our training data to make them realistic\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create 5 sample customers with different profiles\n",
        "sample_customers = pd.DataFrame({\n",
        "    'f0': [12.6, 24.5, 22.0, 15.0, 26.0],  # Engagement level (low to high)\n",
        "    'f1': [10.06, 10.06, 10.06, 10.06, 10.06],\n",
        "    'f2': [8.21, 8.72, 8.64, 8.99, 8.21],\n",
        "    'f3': [4.68, 4.68, -5.48, 1.26, 4.68],\n",
        "    'f4': [10.28, 10.28, 10.28, 10.28, 10.28],\n",
        "    'f5': [4.12, 4.12, 4.12, 4.12, -5.74],\n",
        "    'f6': [-6.70, -2.41, -13.90, 0.29, -25.17],\n",
        "    'f7': [4.83, 4.83, 4.83, 4.83, 11.99],\n",
        "    'f8': [3.97, 3.97, 3.91, 3.93, 3.66],\n",
        "    'f9': [13.19, 13.19, 16.50, 61.90, 13.19],\n",
        "    'f10': [5.30, 5.30, 5.30, 6.47, 5.30],\n",
        "    'f11': [-0.17, -0.17, -0.17, -0.17, -1.09]\n",
        "})\n",
        "\n",
        "# Add descriptive names for clarity\n",
        "customer_names = [\n",
        "    \"Customer A: Low Engagement\",\n",
        "    \"Customer B: Medium Engagement\",\n",
        "    \"Customer C: High Engagement, Diverse Behavior\",\n",
        "    \"Customer D: Very High Activity\",\n",
        "    \"Customer E: Outlier Profile\"\n",
        "]\n",
        "\n",
        "print(\"Sample Customers Created:\")\n",
        "print(\"=\"*80)\n",
        "for i, name in enumerate(customer_names):\n",
        "    print(f\"\\n{name}\")\n",
        "    print(sample_customers.iloc[i:i+1].to_string(index=False))"
      ],
      "metadata": {
        "id": "3nJDN4Vt63F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculate persuadable scores for new customers**\n",
        "\n",
        "This is where the rubber meets the road. For each customer, I'm calculating their Individual Treatment Effect (ITE) using the XGBoost model we trained earlier in Block 2.\n",
        "\n",
        "**The scoring process:**\n",
        "\n",
        "For each customer, I predict TWO scenarios:\n",
        "1. **With ad** (treatment = 1): What's their conversion probability if we show them an ad?\n",
        "2. **Without ad** (treatment = 0): What's their conversion probability if we DON'T show them an ad?\n",
        "\n",
        "**Individual Treatment Effect (ITE) = Difference between these two**\n",
        "\n",
        "**How we classify customers (three simple categories):**\n",
        "- **ITE > 0.001**: Persuadable → TARGET with ads (the ad helps!)\n",
        "- **ITE < -0.001**: Do Not Disturb → EXCLUDE from ads (the ad hurts!)\n",
        "- **ITE ≈ 0** (between -0.001 and +0.001): No Effect → Don't waste budget (ad doesn't matter)\n",
        "\n",
        "**Key insight from the results:**\n",
        "\n",
        "Out of 5 customers, only **Customer C is a persuadable**! Here's the breakdown:\n",
        "\n",
        "**Customer A & B (No Effect - Lost Causes):**\n",
        "- Extremely low conversion probability with or without ads (~0.01%)\n",
        "- ITE ≈ 0 (essentially no difference)\n",
        "- These customers aren't in the market - don't waste money on them\n",
        "\n",
        "**Customer C (PERSUADABLE):**\n",
        "- Without ad: 0.38% conversion probability\n",
        "- With ad: 0.55% conversion probability  \n",
        "- **Treatment effect: +0.17 percentage points**\n",
        "- The ad increases their conversion by 45%! This is where your marketing budget should go.\n",
        "- **This is your target!**\n",
        "\n",
        "**Customer D (No Effect):**\n",
        "- High activity but very low conversion probability (0.14%) with or without ads\n",
        "- ITE ≈ 0 (the ad makes no meaningful difference)\n",
        "- They're browsing but not buying - might be researching or price comparing\n",
        "- Save your budget - the ad won't help\n",
        "\n",
        "**Customer E (No Effect - Borderline):**\n",
        "- Small negative effect (-0.04pp), but within the \"no effect\" threshold\n",
        "- The ad slightly decreases conversion but not enough to classify as \"do not disturb\"\n",
        "- Still not worth targeting - save your budget\n",
        "\n",
        "**The business decision:**\n",
        "\n",
        "Traditional marketing would target Customers C, D, and maybe E (the \"most engaged\" ones).\n",
        "\n",
        "Smart causal marketing targets **only Customer C** - saving 80% of ad spend while capturing the only conversion that actually depends on the ad.\n",
        "\n",
        "\n",
        "**Real-world application:**\n",
        "\n",
        "In your marketing system, you would:\n",
        "1. Score every new customer with this model as they visit your site\n",
        "2. Add high-ITE customers (ITE > 0.001) to your \"Persuadables\" audience in real-time\n",
        "3. Exclude everyone else from paid campaigns\n",
        "4. Result: 4-5x improvement in marketing ROI by targeting only the 12-13% who actually respond\n",
        "\n",
        "This simple scoring process - predict with and without treatment, calculate the difference - is how billion-dollar companies like Uber, Netflix, and Amazon optimize their marketing spend. Now you can do it too."
      ],
      "metadata": {
        "id": "ho8aEsEu8AH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let me score these customers - are they persuadables?\n",
        "# I'll predict conversion probability with and without treatment\n",
        "# Note that we are deliberately setting all 'treatment' values to 1 and then to 0\n",
        "# This is called \"counterfactual prediction\" - we're asking \"what if?\"\n",
        "# We can't observe both (you can't both watch the Ad and not watch), so we use the model to PREDICT the counterfactual.\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERSUADABLE SCORING - Individual Treatment Effects\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, name in enumerate(customer_names):\n",
        "    customer = sample_customers.iloc[i:i+1].copy()\n",
        "\n",
        "    # Scenario 1: Customer sees ad (treatment = 1)\n",
        "    customer_treated = customer.copy()\n",
        "    customer_treated['treatment'] = 1\n",
        "    prob_with_ad = model.predict_proba(customer_treated)[0][1]\n",
        "\n",
        "    # Scenario 2: Customer doesn't see ad (treatment = 0)\n",
        "    customer_control = customer.copy()\n",
        "    customer_control['treatment'] = 0\n",
        "    prob_without_ad = model.predict_proba(customer_control)[0][1]\n",
        "\n",
        "    # Individual Treatment Effect\n",
        "    ite = prob_with_ad - prob_without_ad\n",
        "\n",
        "    # Classify customer - THREE SIMPLE CATEGORIES\n",
        "    if ite > 0.001:\n",
        "        segment = \"PERSUADABLE\"\n",
        "        action = \"TARGET with ads\"\n",
        "        color = \"green\"\n",
        "    elif ite < -0.001:\n",
        "        segment = \"DO NOT DISTURB\"\n",
        "        action = \"EXCLUDE from ads\"\n",
        "        color = \"red\"\n",
        "    else:\n",
        "        segment = \"NO EFFECT\"\n",
        "        action = \"Don't waste budget\"\n",
        "        color = \"gray\"\n",
        "\n",
        "    results.append({\n",
        "        'customer': name,\n",
        "        'prob_without_ad': prob_without_ad,\n",
        "        'prob_with_ad': prob_with_ad,\n",
        "        'ite': ite,\n",
        "        'segment': segment,\n",
        "        'action': action\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"  Conversion probability WITHOUT ad: {prob_without_ad:.4f} ({prob_without_ad*100:.2f}%)\")\n",
        "    print(f\"  Conversion probability WITH ad:    {prob_with_ad:.4f} ({prob_with_ad*100:.2f}%)\")\n",
        "    print(f\"  Treatment Effect (ITE):            {ite:+.4f} ({ite*100:+.2f} percentage points)\")\n",
        "    print(f\"  Segment: {segment}\")\n",
        "    print(f\"  Action: {action}\")\n",
        "\n",
        "# Create summary visualization\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Conversion probabilities with/without ads\n",
        "x = np.arange(len(customer_names))\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(x - width/2, results_df['prob_without_ad'], width, label='Without Ad', alpha=0.8, color='gray')\n",
        "ax1.bar(x + width/2, results_df['prob_with_ad'], width, label='With Ad', alpha=0.8, color='blue')\n",
        "ax1.set_ylabel('Conversion Probability')\n",
        "ax1.set_title('Conversion Probability: With vs Without Ad')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([f'Customer {chr(65+i)}' for i in range(len(customer_names))], rotation=0)\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Treatment effects\n",
        "colors_map = {'PERSUADABLE': 'green', 'DO NOT DISTURB': 'red', 'NO EFFECT': 'gray'}\n",
        "bar_colors = [colors_map[seg] for seg in results_df['segment']]\n",
        "\n",
        "ax2.bar(x, results_df['ite'], color=bar_colors, alpha=0.7)\n",
        "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "ax2.axhline(y=0.001, color='green', linestyle='--', linewidth=1, label='Persuadable threshold')\n",
        "ax2.axhline(y=-0.001, color='red', linestyle='--', linewidth=1, label='Do not disturb threshold')\n",
        "ax2.set_ylabel('Individual Treatment Effect')\n",
        "ax2.set_title('Treatment Effect by Customer')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels([f'Customer {chr(65+i)}' for i in range(len(customer_names))], rotation=0)\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGETING RECOMMENDATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Target with ads: {sum(results_df['segment'] == 'PERSUADABLE')} out of 5 customers\")\n",
        "print(f\"Actively exclude: {sum(results_df['segment'] == 'DO NOT DISTURB')} out of 5 customers\")\n",
        "print(f\"No effect (don't waste budget): {sum(results_df['segment'] == 'NO EFFECT')} out of 5 customers\")"
      ],
      "metadata": {
        "id": "pGbt5APz6-Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why Do We Need DoWhy If We're Just Setting Treatment=0/1 at the End?**\n",
        "\n",
        "Because... They serve different but complementary purposes:\n",
        "\n",
        "**DoWhy (Causal Inference): Validates the Average Causal Effect**\n",
        "- **Answers:** \"Does this ad campaign work ON AVERAGE?\"\n",
        "- **Method:** Adjusts for confounding to get UNBIASED estimate\n",
        "- **Output:** One number (ATE = 0.169pp)\n",
        "- **Why critical:** Without this, we don't know if our approach captures TRUE causation or just correlation\n",
        "\n",
        "**XGBoost with treatment=0/1: Finds Individual Effects**\n",
        "- **Answers:** \"WHO does this campaign work for?\"\n",
        "- **Method:** Predicts counterfactuals (what if treatment changed?)\n",
        "- **Output:** 20,000 individual treatment effects\n",
        "- **Why useful:** Enables precision targeting\n",
        "\n",
        "**The key insight:**\n",
        "DoWhy tells us the XGBoost model IS capturing causal effects (not just confounded correlations). When we see:\n",
        "- DoWhy ATE: 0.169pp\n",
        "- XGBoost average ITE: 0.13pp\n",
        "\n",
        "They're close! This validates our approach. If DoWhy showed no causal effect (or very different number), we'd know the XGBoost predictions are biased by confounding.\n",
        "\n",
        "**Think of it as:**\n",
        "- DoWhy = Quality control (\"Is this method valid?\")\n",
        "- XGBoost = Production (\"Apply to everyone\")\n",
        "\n",
        "Without DoWhy, you're flying blind - you wouldn't know if your individual treatment effects are real or just picking up spurious correlations from confounders."
      ],
      "metadata": {
        "id": "9PpOphNY8Kqk"
      }
    }
  ]
}